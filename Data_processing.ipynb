{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder, LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from math import floor\n",
    "import pickle\n",
    "import xgboost as xg\n",
    "from xgboost.sklearn import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Volumes/Data/Adithya/Kaggle/walmart_salesprediction/walmart-recruiting-store-sales-forecasting/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load datasets\n",
    "features = pd.read_csv(\"datasets/inputdatasets/features.csv\")\n",
    "train = pd.read_csv(\"datasets/inputdatasets/train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize variables\n",
    "seasonality_type = \"week\"\n",
    "event_weeks = [\"2012-07-06\", \"2012-04-06\"]\n",
    "level = [\"Store\", \"Dept\"]\n",
    "validation_split = 0.2\n",
    "model_name = \"xgboost\"   ### choose from linear, ridge, lasso, random forest, gradient boosting\n",
    "ridge_range = [0, 20]\n",
    "lasso_range = [1, 10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'eta': [0.01, 0.015, 0.025, 0.05, 0.1], 'gamma': [0.05, 0.075, 0.1, 0.3, 0.5, 0.7, 0.9, 1], 'min_child_weight': [1, 3, 5, 7, 9], 'max_depth': [3, 6, 9, 12, 15, 18, 21, 24], 'subsample': array([0.6, 0.7, 0.8, 0.9, 1. ]), 'colsample_bytree': array([0.6, 0.7, 0.8, 0.9, 1. ]), 'reg_lambda': [0.01, 0.05, 0.1, 0.5, 1], 'reg_alpha': [0.01, 0.05, 0.1, 0.5, 1]}\n"
     ]
    }
   ],
   "source": [
    "# initialize hyperparameters\n",
    "grid = {}\n",
    "\n",
    "if model_name == \"random forest\":\n",
    "    ## random forest hyperparameters\n",
    "\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)] # Number of trees in random forest\n",
    "    max_features = ['auto', 'sqrt'] # Number of features to consider at every split\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)] # Maximum number of levels in tree\n",
    "    max_depth.append(None)\n",
    "    min_samples_split = [2 , 5, 10, 50] # Minimum number of samples required to split a node\n",
    "    min_samples_leaf = [1, 2, 4, 20] # Minimum number of samples required at each leaf node\n",
    "\n",
    "    # Create the random grid\n",
    "    grid = {'n_estimators': n_estimators,\n",
    "           'max_features': max_features,\n",
    "           'max_depth': max_depth,\n",
    "           'min_samples_split': min_samples_split,\n",
    "           'min_samples_leaf': min_samples_leaf,\n",
    "                   }\n",
    "    \n",
    "elif model_name == \"gradient boosting\":\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)] # Number of trees in random forest\n",
    "    learning_rate = np.linspace(start = 0.1, stop = 0.3, num = 11)\n",
    "    max_features = ['auto', 'sqrt'] # Number of features to consider at every split\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)] # Maximum number of levels in tree\n",
    "    max_depth.append(None)\n",
    "    min_samples_split = [2 , 5, 10, 50] # Minimum number of samples required to split a node\n",
    "    min_samples_leaf = [1, 2, 4, 20] # Minimum number of samples required at each leaf node\n",
    "\n",
    "    # Create the random grid\n",
    "    grid = {'n_estimators': n_estimators,\n",
    "            'learning_rate':learning_rate,\n",
    "           'max_features': max_features,\n",
    "           'max_depth': max_depth,\n",
    "           'min_samples_split': min_samples_split,\n",
    "           'min_samples_leaf': min_samples_leaf,\n",
    "                   }\n",
    "    \n",
    "elif model_name == \"xgboost\":\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "    eta = [0.01, 0.015, 0.025, 0.05, 0.1]\n",
    "    gamma = [0.05, 0.075, 0.1, 0.3, 0.5, 0.7, 0.9, 1]\n",
    "    max_depth = [int(x) for x in np.linspace(start = 3, stop = 24, num = 8)]\n",
    "    min_child_weight = [int(x) for x in np.linspace(start = 1, stop = 9, num = 5)]\n",
    "    subsample = np.linspace(start = 0.6, stop = 1, num = 5)\n",
    "    colsample_bytree = np.linspace(start = 0.6, stop = 1, num = 5)\n",
    "    reg_lambda = [0.01, 0.05, 0.1, 0.5, 1]\n",
    "    reg_alpha = [0.01, 0.05, 0.1, 0.5, 1]\n",
    "    \n",
    "    grid = { 'n_estimators': n_estimators,\n",
    "            'eta': eta,\n",
    "            'gamma': gamma,\n",
    "            'min_child_weight': min_child_weight,\n",
    "            'max_depth': max_depth,\n",
    "            'subsample': subsample,\n",
    "            'colsample_bytree': colsample_bytree,\n",
    "            'reg_lambda': reg_lambda,\n",
    "            'reg_alpha': reg_alpha\n",
    "        \n",
    "    }\n",
    "    \n",
    "# ##\n",
    "# gbcheck = GradientBoostingRegressor()\n",
    "# print(gbcheck.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### merge train and feature datasets\n",
    "def merge_data(main, features):\n",
    "    features_weHoliday = features.drop([\"IsHoliday\"], axis = 1)\n",
    "    inputData = pd.merge(main, features_weHoliday, how = \"left\", on = [\"Store\", \"Date\"])\n",
    "    return inputData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data cleaning \n",
    "## remove nas \n",
    "## remove negative values \n",
    "\n",
    "def data_cleaning(data):\n",
    "    \n",
    "    ##remove nas\n",
    "    data = data.fillna(0)\n",
    "    \n",
    "    ## remove negative values\n",
    "    data = data[data[\"Weekly_Sales\"] >= 0]\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add year month date\n",
    "def add_year_month_week(data):\n",
    "    data[\"year\"], data[\"month\"], data[\"week\"] = [pd.DatetimeIndex(data[\"Date\"]).year, \n",
    "                                             pd.DatetimeIndex(data[\"Date\"]).month,\n",
    "                                             pd.DatetimeIndex(data[\"Date\"]).weekofyear]\n",
    "    data[\"month_str\"] = [\"0\" + x if len(x) == 1 else x for x in data[\"month\"].astype(str)]\n",
    "    \n",
    "    return data\n",
    "# print(inputData.head())\n",
    "# print(inputData[inputData[\"Week\"] == 5].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add trend function\n",
    "def include_trend(data):\n",
    "    label_encoder = LabelEncoder()\n",
    "    data[\"week_trend\"] = label_encoder.fit_transform(data[\"Date\"]) + 1\n",
    "    data[\"year_month\"] = data[\"year\"].astype(str) + \"-\" + data[\"month_str\"].astype(str)\n",
    "    data[\"month_trend\"] = label_encoder.fit_transform(data[\"year_month\"]) + 1\n",
    "    data[\"year_trend\"] = label_encoder.fit_transform(data[\"year\"]) + 1\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add seasonality function\n",
    "def include_seasonality(data, seasonality_type):\n",
    "    \n",
    "    ## initializing binarizer\n",
    "    binarizer = LabelBinarizer()\n",
    "    \n",
    "    ## weekly seasonality\n",
    "    week_encoded = binarizer.fit_transform(data[\"week\"])\n",
    "    week_encoded = np.delete(week_encoded, -1, 1)\n",
    "    column_weeks = np.array([\"week\" + \"_\" + str(x) for x in binarizer.classes_[:len(binarizer.classes_)-1]])\n",
    "    week_encoded = pd.DataFrame(week_encoded,\n",
    "                               columns = column_weeks)\n",
    "    \n",
    "    \n",
    "    ## monthly seasonality\n",
    "    month_encoded = binarizer.fit_transform(data[\"month\"])\n",
    "    month_encoded = np.delete(month_encoded, -1, 1)\n",
    "    column_months = np.array([\"month\" + \"_\" + str(x) for x in binarizer.classes_[:len(binarizer.classes_)-1]])\n",
    "    month_encoded = pd.DataFrame(month_encoded,\n",
    "                                    columns = column_months)\n",
    "    \n",
    "    if seasonality_type == \"week\":\n",
    "        return pd.concat([data.reset_index(), week_encoded.reset_index()], axis = 1)\n",
    "    \n",
    "    else:\n",
    "        return pd.concat([data.reset_index(), month_encoded.reset_index()], axis = 1)\n",
    "\n",
    "\n",
    "# for checking \n",
    "# check = include_seasonality(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add event/spike flags \n",
    "def add_event_flags(data, event_weeks):\n",
    "    data = data.drop([\"IsHoliday\"], axis = 1)\n",
    "    event_weeks = pd.DatetimeIndex(event_weeks)\n",
    "    for week in event_weeks:\n",
    "        data[str(week.year) + \"_\" + str(week.weekofyear) + \"flag\"] = [1 if x == week.date() else 0 for x in data[\"Date\"]]\n",
    "        \n",
    "    return data\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model_name, x_train, y_train, x_val, y_val, ridge_range, lasso_range, random_forest_grid,\n",
    "               hyperparameter_tuning = True):\n",
    "    \n",
    "    ## linear regression\n",
    "    if model_name == \"linear\":\n",
    "        model = LinearRegression()\n",
    "        \n",
    "        results = model.fit(x_train, y_train)\n",
    "        train_predicted = pd.DataFrame(results.predict(x_train), columns = [\"predicted\"])\n",
    "        val_predicted = pd.DataFrame(results.predict(x_val), columns = [\"predicted\"])\n",
    "        min_wape_alpha = 0\n",
    "        \n",
    "        \n",
    "    ## ridge regression    \n",
    "    elif model_name == \"ridge\":\n",
    "        accuracy_dict = {}\n",
    "        for alpha in np.arange(ridge_range[0], ridge_range[1], 0.25):\n",
    "            model = Ridge(alpha = alpha)\n",
    "            results = model.fit(x_train, y_train)\n",
    "            val_predicted = pd.DataFrame(results.predict(x_val), columns = [\"predicted\"])\n",
    "            y_val_df = pd.DataFrame(y_val, columns = [\"Weekly_Sales\"])\n",
    "            val_comparison = pd.concat([y_val_df.reset_index(), val_predicted.reset_index()], axis = 1)\n",
    "            WAPE = abs(val_comparison[\"predicted\"] - val_comparison[\"Weekly_Sales\"]).sum()/val_comparison[\"Weekly_Sales\"].sum()\n",
    "            accuracy_dict[alpha] = WAPE\n",
    "#             print(\"WAPE for \" + str(alpha) + \" is \" + str(WAPE))\n",
    "        \n",
    "        min_wape_alpha = min(accuracy_dict, key = lambda k:accuracy_dict[k])\n",
    "        model = Ridge(alpha = alpha)\n",
    "        results = model.fit(x_train, y_train)\n",
    "        val_predicted = pd.DataFrame(results.predict(x_val), columns = [\"predicted\"])\n",
    "        train_predicted = pd.DataFrame(results.predict(x_train), columns = [\"predicted\"])\n",
    "        \n",
    "        \n",
    "    ## lasso regression    \n",
    "    elif model_name == \"lasso\":\n",
    "        accuracy_dict = {}\n",
    "        for alpha in np.arange(lasso_range[0], lasso_range[1], 0.25):\n",
    "            model = Lasso(alpha = alpha)\n",
    "            results = model.fit(x_train, y_train)\n",
    "            val_predicted = pd.DataFrame(results.predict(x_val), columns = [\"predicted\"])\n",
    "            y_val_df = pd.DataFrame(y_val, columns = [\"Weekly_Sales\"])\n",
    "            val_comparison = pd.concat([y_val_df.reset_index(), val_predicted.reset_index()], axis = 1)\n",
    "            WAPE = abs(val_comparison[\"predicted\"] - val_comparison[\"Weekly_Sales\"]).sum()/val_comparison[\"Weekly_Sales\"].sum()\n",
    "            accuracy_dict[alpha] = WAPE\n",
    "#             print(\"WAPE for \" + str(alpha) + \" is \" + str(WAPE))\n",
    "        \n",
    "        min_wape_alpha = min(accuracy_dict, key = lambda k:accuracy_dict[k])\n",
    "        model = Ridge(alpha = alpha)\n",
    "        results = model.fit(x_train, y_train)\n",
    "        val_predicted = pd.DataFrame(results.predict(x_val), columns = [\"predicted\"])\n",
    "        train_predicted = pd.DataFrame(results.predict(x_train), columns = [\"predicted\"])\n",
    "        \n",
    "        \n",
    "    ## random forests    \n",
    "    elif model_name == \"random forest\":\n",
    "        if hyperparameter_tuning == True:\n",
    "            x_all = np.append(x_train, x_val, axis = 0)\n",
    "            y_all = np.append(y_train, y_val, axis = 0)\n",
    "            tscv = TimeSeriesSplit(n_splits = 3)\n",
    "            random_forest = RandomForestRegressor()\n",
    "            model = RandomizedSearchCV(estimator = random_forest, param_distributions = random_forest_grid,\n",
    "                                      n_iter = 100, cv = tscv, random_state = 42)\n",
    "            results = model.fit(x_all, y_all)\n",
    "    #         model = RandomForestRegressor(n_estimators = 100)\n",
    "    #         results = model.fit(x_train, y_train)\n",
    "            val_predicted = pd.DataFrame(results.predict(x_val), columns = [\"predicted\"])\n",
    "            train_predicted = pd.DataFrame(results.predict(x_train), columns = [\"predicted\"])\n",
    "            min_wape_alpha = 0\n",
    "#         {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 90}    \n",
    "        else:\n",
    "            random_forest = RandomForestRegressor()\n",
    "            model = RandomForestRegressor(n_estimators = 400, min_samples_split = 2, min_samples_leaf = 1,\n",
    "                                         max_features = \"sqrt\", max_depth = 90)\n",
    "            results = model.fit(x_train, y_train)\n",
    "            val_predicted = pd.DataFrame(results.predict(x_val), columns = [\"predicted\"])\n",
    "            train_predicted = pd.DataFrame(results.predict(x_train), columns = [\"predicted\"])\n",
    "            min_wape_alpha = 0\n",
    "        \n",
    "    \n",
    "    ## gradient boosting\n",
    "    elif model_name == \"gradient boosting\":\n",
    "        if hyperparameter_tuning == True:\n",
    "            x_all = np.append(x_train, x_val, axis = 0)\n",
    "            y_all = np.append(y_train, y_val, axis = 0)\n",
    "            tscv = TimeSeriesSplit(n_splits = 3)\n",
    "            gradient_boost = GradientBoostingRegressor()\n",
    "            model = RandomizedSearchCV(estimator = gradient_boost, param_distributions = grid,\n",
    "                                      n_iter = 100, cv = tscv, random_state = 42)\n",
    "            results = model.fit(x_all, y_all)\n",
    "    #         model = RandomForestRegressor(n_estimators = 100)\n",
    "    #         results = model.fit(x_train, y_train)\n",
    "            val_predicted = pd.DataFrame(results.predict(x_val), columns = [\"predicted\"])\n",
    "            train_predicted = pd.DataFrame(results.predict(x_train), columns = [\"predicted\"])\n",
    "            min_wape_alpha = 0\n",
    "#         {'n_estimators': 400, 'min_samples_split': 50, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 100, 'learning_rate': 0.18}\n",
    "        else:\n",
    "            model = GradientBoostingRegressor(n_estimators = 400, learning_rate = 0.18,\n",
    "                                              min_samples_split = 50, min_samples_leaf = 1,\n",
    "                                         max_features = \"sqrt\", max_depth = 100)\n",
    "            results = model.fit(x_train, y_train)\n",
    "            val_predicted = pd.DataFrame(results.predict(x_val), columns = [\"predicted\"])\n",
    "            train_predicted = pd.DataFrame(results.predict(x_train), columns = [\"predicted\"])\n",
    "            min_wape_alpha = 0\n",
    "            \n",
    "            \n",
    "    elif model_name == \"xgboost\":\n",
    "        if hyperparameter_tuning == True:\n",
    "            x_all = np.append(x_train, x_val, axis = 0)\n",
    "            y_all = np.append(y_train, y_val, axis = 0)\n",
    "            tscv = TimeSeriesSplit(n_splits = 3)\n",
    "            xgb = XGBRegressor(objective = \"reg:squarederror\")\n",
    "#             model = RandomizedSearchCV(estimator = xgb, param_distributions = grid,\n",
    "#                                        cv = tscv, random_state = 42)\n",
    "            model = GridSearchCV(estimator = xgb, param_grid = grid,\n",
    "                                       cv = tscv)\n",
    "            results = model.fit(x_all, y_all)\n",
    "            val_predicted = pd.DataFrame(results.predict(x_val), columns = [\"predicted\"])\n",
    "            train_predicted = pd.DataFrame(results.predict(x_train), columns = [\"predicted\"])\n",
    "            min_wape_alpha = 0\n",
    "#         'subsample': 1.0, 'reg_lambda': 0.05, 'reg_alpha': 0.01, 'n_estimators': 1200, 'min_child_weight': 1, 'max_depth': 3, 'gamma': 0.3, 'eta': 0.015, 'colsample_bytree': 0.9\n",
    "        else:\n",
    "            model = xg.XGBRegressor(objective = \"reg:squarederror\",\n",
    "                                    subsample = 1,\n",
    "                                    n_estimators = 1200,\n",
    "                                   reg_lambda = 0.05,\n",
    "                                   reg_alpha = 0.01,\n",
    "                                   min_child_weight = 1,\n",
    "                                   max_depth = 3,\n",
    "                                   gamma = 0.3,\n",
    "                                   eta = 0.015,\n",
    "                                   colsample_bytree = 1)\n",
    "            results = model.fit(x_train, y_train)\n",
    "            val_predicted = pd.DataFrame(results.predict(x_val), columns = [\"predicted\"])\n",
    "            train_predicted = pd.DataFrame(results.predict(x_train), columns = [\"predicted\"])\n",
    "            min_wape_alpha = 0\n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "    else:\n",
    "        return \"Error\"\n",
    "    \n",
    "    return train_predicted, val_predicted, min_wape_alpha, results\n",
    "            \n",
    "        \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculate error metrics - rmse, wape\n",
    "\n",
    "def error_metrics(data):\n",
    "    \n",
    "    ##wape\n",
    "    data[\"absolute_error\"] = abs(data[\"predicted\"] - data[\"Weekly_Sales\"])\n",
    "    WAPE = data[\"absolute_error\"].sum()/data[\"Weekly_Sales\"].sum()\n",
    "    data[\"WAPE\"] = WAPE\n",
    "    \n",
    "    ##rmse\n",
    "    RMSE = np.sqrt(np.square(data[\"absolute_error\"]).mean())\n",
    "    data[\"RMSE\"] = RMSE\n",
    "    \n",
    "    ##wmae\n",
    "    WMAE = np.sum(data[\"absolute_error\"] * data[\"Weekly_Sales\"])/np.sum(data[\"Weekly_Sales\"])\n",
    "    data[\"WMAE\"] = WMAE\n",
    "    \n",
    "    return data\n",
    "\n",
    "## f\n",
    "# check_data, wape, rmse, wmae = error_metrics(trained_data)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_run(data, level, model_name, ridge_range, lasso_range, random_forest_grid,\n",
    "              test_run = True, validation_split = 0.2):\n",
    "    \n",
    "    \n",
    "    columns_to_drop = [\"Store\", \"Dept\", \"Date\", \"month_str\", \"year_month\", \"week_trend\", \"year_trend\"]\n",
    "    target = \"Weekly_Sales\"\n",
    "    full_dataset = pd.DataFrame()\n",
    "    i = 0\n",
    "    \n",
    "    if test_run == True:\n",
    "        check_data = data[(data[\"Store\"] < 2 )  & (data[\"Dept\"] < 2)]\n",
    "        grouped = check_data.groupby([\"Store\", \"Dept\"])\n",
    "    else:\n",
    "        grouped = data.groupby([\"Store\", \"Dept\"])\n",
    "        \n",
    "    for name, group in grouped:\n",
    "        temp_data = data[(data[\"Store\"] == name[0]) & (data[\"Dept\"] == name[1])]\n",
    "        print(name[0], name[1])\n",
    "        if len(temp_data) >= 52:\n",
    "            y_dataframe = pd.DataFrame(temp_data[target])\n",
    "            y_columns = y_dataframe.columns \n",
    "            y = np.array(y_dataframe)\n",
    "\n",
    "            temp_data = temp_data.drop(target, axis = 1)\n",
    "            x_dropped = temp_data[columns_to_drop]\n",
    "            x_dataframe = temp_data.drop(columns_to_drop, axis = 1)\n",
    "            x_columns = x_dataframe.columns\n",
    "            x = np.array(x_dataframe)\n",
    "\n",
    "\n",
    "            ## train, validation split\n",
    "            x_train, x_val, y_train, y_val = train_test_split(x, y, test_size = validation_split)\n",
    "            \n",
    "            train_predicted, val_predicted, min_wape_alpha, model = model_train(model_name, x_train, y_train, x_val, y_val, ridge_range, lasso_range, random_forest_grid)\n",
    "\n",
    "\n",
    "            print(np.mean(train_predicted[\"predicted\"]))\n",
    "            print(np.mean(val_predicted[\"predicted\"]))\n",
    "            \n",
    "            x_train_df = pd.DataFrame(x_train, columns = x_columns)\n",
    "            y_train_df = pd.DataFrame(y_train, columns = y_columns)\n",
    "            x_val_df = pd.DataFrame(x_val, columns = x_columns)\n",
    "            y_val_df = pd.DataFrame(y_val, columns = y_columns)\n",
    "\n",
    "            train_wpredicted = pd.concat([x_train_df, y_train_df, train_predicted], axis = 1)\n",
    "            train_wpredicted[\"tag\"] = \"train\"\n",
    "            train_wpredicted = error_metrics(train_wpredicted)\n",
    "            \n",
    "            val_wpredicted = pd.concat([x_val_df, y_val_df, val_predicted], axis = 1)\n",
    "            val_wpredicted[\"tag\"] = \"validation\"\n",
    "            val_wpredicted = error_metrics(val_wpredicted)\n",
    "            print(np.mean(val_wpredicted[\"WAPE\"]))\n",
    "            train_and_val = pd.concat([train_wpredicted, val_wpredicted])\n",
    "            train_and_val_wdropped = pd.concat([train_and_val.reset_index(), x_dropped.reset_index()], axis = 1)\n",
    "\n",
    "            full_dataset = full_dataset.append(train_and_val_wdropped)\n",
    "            i += 1\n",
    "            print(i , \" iterations complete\")\n",
    "        \n",
    "    return full_dataset, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (<ipython-input-17-fe8f80f791ac>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-fe8f80f791ac>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def model_run(data, level, test_run = True, validation_split = 0.2, ridge_range, lasso_range):\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "def model_run(data, level, test_run = True, validation_split = 0.2, ridge_range, lasso_range):\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    \n",
    "    columns_to_drop = [\"Store\", \"Dept\", \"Date\", \"month_str\", \"year_month\", \"week_trend\", \"year_trend\"]\n",
    "    target = \"Weekly_Sales\"\n",
    "    full_dataset = pd.DataFrame()\n",
    "    i = 0\n",
    "    \n",
    "    if test_run == True:\n",
    "        check_data = data[(data[\"Store\"] < 4 )  & (data[\"Dept\"] < 4)]\n",
    "        grouped = check_data.groupby([\"Store\", \"Dept\"])\n",
    "    else:\n",
    "        grouped = data.groupby([\"Store\", \"Dept\"])\n",
    "        \n",
    "    for name, group in grouped:\n",
    "        temp_data = data[(data[\"Store\"] == name[0]) & (data[\"Dept\"] == name[1])]\n",
    "        print(name[0], name[1])\n",
    "        if len(temp_data) >= 52:\n",
    "            y_dataframe = pd.DataFrame(temp_data[target])\n",
    "            y_columns = y_dataframe.columns \n",
    "            y = np.array(y_dataframe)\n",
    "\n",
    "            temp_data = temp_data.drop(target, axis = 1)\n",
    "            x_dropped = temp_data[columns_to_drop]\n",
    "            x_dataframe = temp_data.drop(columns_to_drop, axis = 1)\n",
    "            x_columns = x_dataframe.columns\n",
    "            x = np.array(x_dataframe)\n",
    "\n",
    "\n",
    "            ## train, validation split\n",
    "            x_train, x_val, y_train, y_val = train_test_split(x, y, test_size = validation_split)\n",
    "\n",
    "\n",
    "            x_train_wconstant = sm.add_constant(x_train)\n",
    "            x_val_wconstant = sm.add_constant(x_val)\n",
    "\n",
    "            model = sm.OLS(y_train, x_train_wconstant)\n",
    "\n",
    "            results = model.fit()\n",
    "\n",
    "            train_predicted = pd.DataFrame(results.fittedvalues, columns = [\"predicted\"])\n",
    "            print(x_val_wconstant.shape)\n",
    "            print(results.predict(x_val_wconstant).shape)\n",
    "            val_predicted = pd.DataFrame(results.predict(x_val_wconstant), columns = [\"predicted\"])\n",
    "\n",
    "            x_train_df = pd.DataFrame(x_train, columns = x_columns)\n",
    "            y_train_df = pd.DataFrame(y_train, columns = y_columns)\n",
    "            x_val_df = pd.DataFrame(x_val, columns = x_columns)\n",
    "            y_val_df = pd.DataFrame(y_val, columns = y_columns)\n",
    "\n",
    "            train_wpredicted = pd.concat([x_train_df, y_train_df, train_predicted], axis = 1)\n",
    "            train_wpredicted[\"tag\"] = \"train\"\n",
    "            train_wpredicted = error_metrics(train_wpredicted)\n",
    "            \n",
    "            val_wpredicted = pd.concat([x_val_df, y_val_df, val_predicted], axis = 1)\n",
    "            val_wpredicted[\"tag\"] = \"validation\"\n",
    "            val_wpredicted = error_metrics(val_wpredicted)\n",
    "\n",
    "            train_and_val = pd.concat([train_wpredicted, val_wpredicted])\n",
    "            train_and_val_wdropped = pd.concat([train_and_val.reset_index(), x_dropped.reset_index()], axis = 1)\n",
    "\n",
    "            full_dataset = full_dataset.append(train_and_val_wdropped)\n",
    "            i += 1\n",
    "            print(i , \" iterations complete\")\n",
    "        \n",
    "    return full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    input_data = merge_data(train, features)\n",
    "    input_data = data_cleaning(input_data)\n",
    "    input_data = add_year_month_week(input_data)\n",
    "    input_data = include_trend(input_data)\n",
    "    input_data = include_seasonality(input_data,\n",
    "                                           seasonality_type)\n",
    "    input_data = add_event_flags(input_data, event_weeks)\n",
    "    \n",
    "    input_data.to_csv(\"datasets/processeddatasets/input_data.csv\")\n",
    "    \n",
    "    \n",
    "    trained_data, model = model_run(input_data, level, model_name, ridge_range, lasso_range, grid, test_run = True)\n",
    "    \n",
    "#     trained_data.to_csv(\"datasets/processeddatasets/trained_data.csv\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01716813790581898\n",
      "0.14336897473748045\n",
      "374.65554936248316\n",
      "4132.081014542145\n"
     ]
    }
   ],
   "source": [
    "# model_filename = \"testmodel_gb.sav\"\n",
    "# pickle.dump(model, open(model_filename, 'wb'))\n",
    "print(np.mean(trained_data[trained_data[\"tag\"] == \"train\"][\"WAPE\"]))\n",
    "print(np.mean(trained_data[trained_data[\"tag\"] == \"validation\"][\"WAPE\"]))\n",
    "print(np.mean(trained_data[trained_data[\"tag\"] == \"train\"][\"WMAE\"]))\n",
    "print(np.mean(trained_data[trained_data[\"tag\"] == \"validation\"][\"WMAE\"]))\n",
    "\n",
    "# default random forest settings\n",
    "# train wape - 0.054124083923314886\n",
    "# val wape - 0.16467224121602228\n",
    "\n",
    "## tuned random forest \n",
    "# train wape - 0.054418925093969624\n",
    "# val wape - 0.14835607061222403\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
